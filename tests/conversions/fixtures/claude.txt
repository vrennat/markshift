Here's a summary of the key concepts:

## Overview

The system uses a **pipeline architecture** where data flows through several stages:

1. **Input parsing** - raw text is tokenized
2. **Transformation** - tokens are converted to an AST
3. **Output generation** - the AST is serialized

### Code Example

```python
def transform(tokens):
    ast = build_tree(tokens)
    return serialize(ast)
```

The time complexity is $O(n \log n)$ for most inputs, though worst case is $O(n^2)$.

> **Note:** This approach works well for _structured_ data but may struggle with ~~unstructured~~ semi-structured content.

Here's a comparison:

| Approach | Speed | Memory |
|----------|-------|--------|
| Recursive | Fast | High |
| Iterative | Medium | Low |

- First item with **bold text**
- Second item with `inline code`
  - Nested item
  - Another nested item
- Third item
